{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe8b475d",
   "metadata": {},
   "source": [
    "<h1><center>Forward Propagation in Neural Networks</center></h1>\n",
    "<h3><center>Reading time: 10 minutes</center></h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b31acd80",
   "metadata": {},
   "source": [
    "![Example of a neural network](./Images/neuralnetwork.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c9a39da",
   "metadata": {},
   "source": [
    "*Forward propagation in neural networks is the process of updating/calculating values in individual neurons in our neural network. When performing forward propagation and back propagation, we essentially have developed the basics for our neural network. With the use of these two methods we can easily train a neural network. Forward propagation is the method of calculating the outputs of our neural network, this means we take some inputs and send them through the hidden layers of our neural network to calculate our output. Backpropagation is the mathematical method of looking at our output to calculate loss and moving backwards (right to left) through our neural network and updating the weights and biases for each neuron to minimize this loss.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21bd273e",
   "metadata": {},
   "source": [
    "**In this walkthrough we will develop a simple neural network by hand and run only the forward pass to gain a strong foundation on what this step of training looks like.**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49722478",
   "metadata": {},
   "source": [
    "<h2><center>Mathematical Foundation - Preactivation</center></h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cede08ad",
   "metadata": {},
   "source": [
    "*Forward propagation is a 2-step method for each neuron. First we will do what's called \"preactivation\" and then we will do what's called \"activation\". This is done for each neuron in the hidden and output layers. Preactivation is just the pure calculation of neuron values. What this means is each neurons value after the input layer is calculated by taking the sum of multiplying the weight (edge) and the incoming neuron value. Most times you will also add a bias to each neuron calculation.*\n",
    "\n",
    "**_Preactivation is truly just a weighted sum of inputs. This can be derived as_**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8744ed0b",
   "metadata": {},
   "source": [
    "<center>$\\sum{w_ix_i} + b_l$</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a4bbb5",
   "metadata": {},
   "source": [
    "*The edges connecting each neuron to another is considered the weight of those neurons connections. Performing the sum of all connected edges times their associated neuron results in the output for a given neuron. Further, each layer normally has a bias which is added to the weighted sum at the very end. In this notebook we will keep the bias at 0, but in most neural networks you will have a shared bias for the entire layer or even a bias for each neuron in a layer. Let's do an example*\n",
    "\n",
    "![Example of a neural network](./Images/neuralnetwork.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42033163",
   "metadata": {},
   "source": [
    "**Let's compute the value for $h_1$. Our given derivation is $\\sum{w_ix_i} + b_l$, as such we see that the value for $h_1$ is $h_1 = i_1w_1 + i_2w_2 + b_1$. Let's derive each neuron manually using this equation to ensure the step of preactivation is very clear**\n",
    "\n",
    "* $h_1 = i_1w_1 + i_2w_2 + b_1 $\n",
    "* $h_2 = i_1w_3 + i_2w_4 + b_1 $\n",
    "* $output = h_1w_5 + h_2w_6 + b_2$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bce0b4b3",
   "metadata": {},
   "source": [
    "<h2><center>Mathematical Foundation - Activation</center></h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95d95871",
   "metadata": {},
   "source": [
    "\n",
    "**Great, preactivation is clearly just a weighted sum of the inputs of any given neuron! However, this generally isn't enough. The next step is to apply an activation function to each neuron after calculating its value through preactivation. A common activation function is sigmoid (this is also a very easily understood one). Sigmoid basically takes our value from our preactivation steps and converts it into the following interval    \n",
    "[0, 1]. Sigmoid is simply the fraction of $1 + e$ to the power of our preactivation value**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfc2dc09",
   "metadata": {},
   "source": [
    "<center><h2>$\\frac{1}{1+e^{-z}}$</h2></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e49feadb",
   "metadata": {},
   "source": [
    "**All we need to do to finish the process of forward propagation is apply this activation function after we do preactivation on each neuron. Let's do the same thing as before and do this manually for each neuron**\n",
    "\n",
    "* $Preh_1 = i_1w_1 + i_2w_2 + b_1 $\n",
    "    * $h_1 = \\frac{1}{1+e^{-Preh_1}}$\n",
    "* $Preh_2 = i_1w_3 + i_2w_4 + b_1 $\n",
    "    * $h_2 = \\frac{1}{1+e^{-Preh_2}}$\n",
    "* $Preoutput = h_1w_5 + h_2w_6 + b_2$\n",
    "    * $output = \\frac{1}{1+e^{-Preoutput}}$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b7ad397",
   "metadata": {},
   "source": [
    "<h2><center>Mathematical Foundation - Numerical Example</center></h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d11fc4f0",
   "metadata": {},
   "source": [
    "**Let's do an example with numerical values. Traditionally, we initialize the bias values for each layer to be 0 and the weights to be within a range of [0,1] (this can vary, but for this example we will randomly initialize all weights between 0 - 1). Obviously, the values for the input neurons will depend on our dataset, so we will just take random inputs as well. Let's take both bias terms to be 0 and $w_1 = 0.1, w_2 = 0.6, w_3 = 0.9, w_4 = 1, w_5 = 0.1, w_6 = 1, i_1 = 1, i_2 = 0$.**\n",
    "\n",
    "**Now we can do preactivation and activation for each neuron.**\n",
    "\n",
    "Preactivation -> Activation\n",
    "\n",
    "* $h_1 = 1*0.1 + 0*0.6 + 0  = 0.1$\n",
    "    * $h_1 = \\frac{1}{1+e^{-0.1}} = 0.525$\n",
    "    \n",
    "Preactivation -> Activation\n",
    "\n",
    "* $h_2 = 1*0.9 + 0*1 + 0  = 0.9$\n",
    "    * $h_2 = \\frac{1}{1+e^{-0.9}} = 0.711$\n",
    "\n",
    "Preactivation -> Activation\n",
    "\n",
    "* $output = 0.525*0.1 + 0.711*1 + 0 = 0.7635 $\n",
    "    * $output = \\frac{1}{1+e^{-0.7635}} = 0.682$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5fa4e11",
   "metadata": {},
   "source": [
    "<h2><center>Code Example</center></h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a4f8a29",
   "metadata": {},
   "source": [
    "**Let's continue to use the first diagram. We know we have 6 weights, 2 inputs, and 3 biases. Remember, we really just randomly initialize the weights and set the biases to zero**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e0547bfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output for inputs [1, 0]: 0.6821017378863259\n",
      "Output for inputs [0, 1]: 0.6890376797993403\n",
      "Output for inputs [1, 1]: 0.7184346752219324\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Sigmoid\n",
    "def sigmoid(x):\n",
    "     return 1.0/(1.0 + np.exp(-x))\n",
    "\n",
    "# Our six weights\n",
    "Weights = [0.1, 0.6,0.9,1,0.1,1]\n",
    "\n",
    "# All bias terms in this example are zero\n",
    "Bias = 0\n",
    "\n",
    "# Let's do 3 iterations of 3 different inputs\n",
    "inputs = [[1,0],[0,1],[1,1]]\n",
    "\n",
    "\n",
    "# Three examples of forward propagation. The first one is the same example we did above\n",
    "for input in inputs:\n",
    "    i1 = input[0]\n",
    "    i2 = input[1]\n",
    "    \n",
    "    # h1 & h2 neuron (above diagram)\n",
    "    h1 = i1*Weights[0] + i2*Weights[1] + 0\n",
    "    h1 = sigmoid(h1)\n",
    "                \n",
    "    h2 = i1*Weights[2] + i2*Weights[3] + 0\n",
    "    h2 = sigmoid(h2)\n",
    "    \n",
    "    output = h1*Weights[4] + h2*Weights[5] + 0\n",
    "    output = sigmoid(output)\n",
    "    \n",
    "    print(\"Output for inputs {}: {}\".format(input, output))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07131520",
   "metadata": {},
   "source": [
    "<h2><center>Code Example - Numpy</center></h2>\n",
    "\n",
    "<b><p>With Numpy we can do this much cleaner by taking advantage of matrix multiplication. Here we see that we have a array that holds the weights going into each neuron. Each layer must have a set of weights for each neuron. This set of weights for each neuron will be the size of the previous layers amount of neurons. Here for our hidden layer we have 2 input neurons. This means we are required to have 2 weights going into each neuron in our hidden layer. Since we have 2 neurons in the hidden layer our weights are simply 2 sets of 2 (2x2). Our output layer is one neuron so we need one set of weights and the amount of weights we need are determined by the neuron count in the previous layer, this is clearly 2. Therefore we need 1 set of 2 weights for our final layer (output). Let's look how this makes sense in Linear Algebra</p></b>\n",
    "\n",
    "\n",
    "$\\begin{pmatrix}x_1&x_2\\end{pmatrix}\\begin{pmatrix}w_1&w_3\\\\ w_2&w_4\\end{pmatrix}=\\begin{pmatrix}x_1w_1+x_2w_2&x_1w_3+x_2w_4\\end{pmatrix}$\n",
    "\n",
    "If we think of this first matrix as our weights and the second matrix as the neuron values we can simply yield the result for each neuron in the hidden layer through one matrix multiplication. Note that each row indicates one neuron\n",
    "\n",
    "$\\begin{pmatrix}1&0\\end{pmatrix}\\begin{pmatrix}0.1&0.9\\\\ 0.6&1\\end{pmatrix}=\\begin{pmatrix}0.1&0.9\\end{pmatrix}$\n",
    "\n",
    "We can also do this for the output layer. First make sure to take the sigmoid of this matrix (0.1 and 0.9):\n",
    "\n",
    "$\\begin{pmatrix}0.5249&0.71094\\end{pmatrix}\\begin{pmatrix}0.1\\\\ 1\\end{pmatrix}=\\begin{pmatrix}0.76343\\end{pmatrix}$\n",
    "\n",
    "Clearly, after we do the activation function (sigmoid) on this result we get the same output value as the previous example: ~0.682"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "02fba146",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output for inputs [1 0]: 0.6821017378863259\n",
      "Output for inputs [0 1]: 0.6890376797993403\n",
      "Output for inputs [1 1]: 0.7184346752219324\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1.0/(1.0 + np.exp(-x))\n",
    "\n",
    "# Weights for hidden layer\n",
    "w1 = np.array([[0.1, 0.6],[0.9,1.0]]).T\n",
    "\n",
    "# Weights for output layer\n",
    "w2 = np.array([0.1,1.0]).T\n",
    "\n",
    "# All bias terms in this example are zero\n",
    "Bias = 0\n",
    "\n",
    "# Let's do 3 iterations of 3 different inputs\n",
    "inputs = np.array([[1,0],[0,1],[1,1]])\n",
    "\n",
    "for input in inputs:\n",
    "    # h1 & h2 neuron from above diagram\n",
    "    hidden = sigmoid(input @ w1)\n",
    "        \n",
    "    # ouput neuron from above diagram\n",
    "    output = sigmoid(hidden @ w2)\n",
    "        \n",
    "    print(\"Output for inputs {}: {}\".format(input, output))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66e71ff3",
   "metadata": {},
   "source": [
    "<h2><center>Robust Code</center></h2>\n",
    "\n",
    "<b><p>Now that we understand how to do forward propagation by hand using matrix multiplication, let's develop a more robust system from scratch. Note** if we use PyTorch, this is all handled for us, but this is great for learning and understanding how to develop neural networks from scratch!</p></b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "95968d91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.38609474])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "class NeuralNetwork:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.W1 = np.array(np.random.randn(2,2))\n",
    "        self.W2 = np.array(np.random.randn(2,1))\n",
    "        self.B1 = np.zeros((1,2))\n",
    "        self.B2 = np.zeros((1,1))\n",
    "        \n",
    "    def sigmoid(self, neurons):\n",
    "        return 1.0/(1.0 + np.exp(-neurons))\n",
    "    \n",
    "    def forward(self, input):\n",
    "        hidden = self.sigmoid(input @ self.W1)\n",
    "        output = self.sigmoid(hidden @ self.W2)\n",
    "        return output\n",
    "    \n",
    "net = NeuralNetwork()\n",
    "train = np.array([1,0])\n",
    "net.forward(train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f875948",
   "metadata": {},
   "source": [
    "<h2><center>Production Code</center></h2>\n",
    "\n",
    "<b><p>We need a much more robust code base for forward propagation. Something that allows us to tune our layers and inputs much more. With this code base we can now tune our neural network as we please. We can use more hidden neurons, more hidden layers, more output neurons, and more input neurons.</p></b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "091186e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "# Each layer in our neural network\n",
    "class NeuralLayer:\n",
    "    # Randomly initialize weights and biases based off of layer size\n",
    "    def __init__(self, input_neurons, output_neurons):\n",
    "        self.weights = np.random.randn(input_neurons, output_neurons)\n",
    "        self.bias = np.zeros((1,output_neurons))\n",
    "\n",
    "    # Two different activations, sigmoid by default\n",
    "    def sigmoid(self, neurons):\n",
    "        return 1.0/(1.0 + np.exp(-neurons))\n",
    "    \n",
    "    def relu(self, neurons):\n",
    "        return neuron * (neurons > 0)\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, input, activation):\n",
    "        if activation == 'sigmoid':\n",
    "            return self.sigmoid(input @ self.weights + self.bias)\n",
    "        else:\n",
    "            return self.relu(input @ self.weights + self.bias)\n",
    "\n",
    "\n",
    "# Our neural net\n",
    "class NeuralNetwork:\n",
    "    \n",
    "    # Dynamically create all layers \n",
    "    def __init__(self, input_neurons, hidden_neurons, layer_count, output_neurons = 1):\n",
    "        \n",
    "        # Used to ensure input neurons match inputted data\n",
    "        self.neuron_safety = input_neurons\n",
    "        \n",
    "        # Assert we have a input and output layer at the least\n",
    "        assert layer_count >= 2 and output_neurons >= 1\n",
    "        \n",
    "        # Input layer\n",
    "        self.layers = [NeuralLayer(input_neurons, hidden_neurons)]\n",
    "                \n",
    "        # Hidden Layers\n",
    "        for i in range(layer_count - 2):\n",
    "            self.layers.append(NeuralLayer(hidden_neurons, hidden_neurons))\n",
    "            \n",
    "        # Output layer\n",
    "        self.layers.append(NeuralLayer(hidden_neurons, output_neurons))\n",
    "    \n",
    "    # Forward pass for each layer\n",
    "    def forward(self, inp, activation = 'sigmoid'):\n",
    "        \n",
    "        assert inp.shape[0] == self.neuron_safety\n",
    "        \n",
    "        \n",
    "        for layer in self.layers:\n",
    "            inp = layer.forward(inp, activation)\n",
    "            \n",
    "        return inp "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "429d0a43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.82857646]]\n"
     ]
    }
   ],
   "source": [
    "# Create a neural network with 3 inputs, 6 hidden neurons in each layer, and 5 layers \n",
    "net = NeuralNetwork(3,6,5)\n",
    "\n",
    "# Input data\n",
    "X = np.array(([1,0,6]))\n",
    "\n",
    "X = net.forward(X)\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "771a66b6",
   "metadata": {},
   "source": [
    "<h2><center>Summary</center></h2>\n",
    "\n",
    "\n",
    "**Forward propagation is simply the idea of calculating values for neurons. More so, it's simply the idea of moving from the input layer to the output layer. This is how our neural networks calculates its predictions. We've seen in this notebook how to actually do forward propagation through preactivation and activation. The next step is to use back propagation to update our weights and biases until our model performs well!**\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
